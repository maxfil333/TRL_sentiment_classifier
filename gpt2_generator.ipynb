{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b12878c6-f30c-42f5-963e-cde54ab220fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c03d96-9796-4c7e-ac23-df432e673ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab14af1-638c-4253-a29c-868e9517b4f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40958d94-2c5d-4a5b-ab10-d6f3e49f96a1",
   "metadata": {},
   "source": [
    "### imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac6371c-4efa-48d3-b4ed-7a1f8de99884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a1b142-f433-4242-878c-5d07fb5178f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 24895\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 24872\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 49776\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset('imdb')\n",
    "ds = ds.rename_columns({'text': 'review', 'label': 'sentiment'})\n",
    "ds = ds.filter(lambda x: len(x[\"review\"])>200, batched=False)\n",
    "ds = ds.shuffle(seed=1)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975c8099-59f1-4272-9b6b-59201a9057bf",
   "metadata": {},
   "source": [
    "### default model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e6ab94-9bfa-4bd1-839d-b1fa1793bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b69e6-1c67-4438-b001-bb3c18cc540d",
   "metadata": {},
   "source": [
    "#### < test ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea1d5d1-2a6d-42ae-939f-6a9a31824116",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'the largest city in USA is'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2919a266-40e1-4f83-ad57-d6803187bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1169, 4387, 1748,  287, 4916,  318]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(text, return_tensors = 'pt')['input_ids']\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32a17809-0223-480d-b5c2-565684370aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1169,  4387,  1748,   287,  4916,   318,   287,   262,  1294,    11,\n",
       "           475,   612,   389,   991,   257,  1271,   286,  4736,   326,   423,\n",
       "           257,  4025,  3265,   621, 19170,    13,   198,   198,  4342,   318,\n",
       "           257,  1351,   286,   262,  1353,   838,  4736,   287,   262,  1294,\n",
       "           351,   262,  4387,  3265,   286,   661,    13,   198,   198,    16,\n",
       "            13,   968,  1971,  2254,   198,   198,    17,    13,  5401,  5652,\n",
       "           198,   198,    18,    13,  2986,  6033,   198,   198,    19,    13,\n",
       "          8437,   198,   198,    20,    13,  2986,  9500,   198,   198,    21,\n",
       "            13,  4842,   198,   198,    22,    13,   968,  1971,  2254,   198,\n",
       "           198,    23,    13,  2986,  6033,   198,   198,    24,    13,  8437]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(input_ids, \n",
    "                    do_sample=True,\n",
    "                    num_beams=2,\n",
    "                    temperature=1.1,\n",
    "                    top_p=0.9,\n",
    "                    max_length=100,\n",
    "                    )\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "984fbe4a-159c-4790-b3ee-8d5c30e8d7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the largest city in USA is in the US, but there are still a number of cities that have a larger population than NYC.\\n\\nHere is a list of the top 10 cities in the US with the largest population of people.\\n\\n1. New York City\\n\\n2. Los Angeles\\n\\n3. San Francisco\\n\\n4. Miami\\n\\n5. San Diego\\n\\n6. Chicago\\n\\n7. New York City\\n\\n8. San Francisco\\n\\n9. Miami']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text = list(map(tokenizer.decode, out))\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ce6e7-0800-4e81-b12f-5c3b71baa3ee",
   "metadata": {},
   "source": [
    "#### ...test >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997eb797-9aae-49f1-b031-b3ab42fd8813",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "355e7298-04c2-4d3b-8655-ba3fd8ef15cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 24895\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 24872\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['review', 'sentiment', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 49776\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_func(examples):\n",
    "    return tokenizer(examples['review'], truncation=True, max_length = 256)\n",
    "\n",
    "tokenized_ds = ds.map(preprocess_func, batched=True)\n",
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37a2c5b3-d354-438f-8ad5-e31ee146ac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lengths.max()=256.0\n",
      "lengths.min()=39.0\n",
      "lengths.mean()=203.6788913436433\n"
     ]
    }
   ],
   "source": [
    "lengths = np.zeros(len(tokenized_ds['train']['input_ids']))\n",
    "for i, ids in enumerate(tokenized_ds['train']['input_ids']):\n",
    "    lengths[i] = len(ids)\n",
    "print(f'{lengths.max()=}')\n",
    "print(f'{lengths.min()=}')\n",
    "print(f'{lengths.mean()=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8609e0a-2f5a-4d44-ac0b-c75e291a555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = tokenized_ds.remove_columns(['review', 'sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde3355e-f9ba-4729-b9bd-fa0b26ac0b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds.set_format('torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6def0969-029a-45fb-a609-13e38eb42d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenized_ds['train']['input_ids'][0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30874517-2bba-453b-bfdc-c5e2c2f69271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([204])\n",
      "torch.Size([143])\n",
      "torch.Size([256])\n",
      "torch.Size([127])\n",
      "torch.Size([151])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for i, train_ds in enumerate(train_dataset):\n",
    "    print(train_ds.shape)\n",
    "    if i == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "855e871e-3151-4c58-beb1-11b3e838a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc76d32c-5092-466b-a9c1-bcb5eecba27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732398b9-bd8d-49e9-992d-5c6341bcc5ac",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54df6ad5-3699-4f8c-b5bf-890a5bee0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned\", # The output directory\n",
    "    overwrite_output_dir=True, # Overwrite the content of the output dir\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=8, # batch size for training\n",
    "    per_device_eval_batch_size=8,  # batch size for evaluation\n",
    "    warmup_steps=10, # number of warmup steps for learning rate scheduler\n",
    "    # gradient_accumulation_steps=16, # to make \"virtual\" batch size larger\n",
    "    logging_steps=50\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    optimizers = (torch.optim.AdamW(model.parameters(),lr=1e-5), None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7df8543-7870-4b71-9f11-406f4654cac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxfil333\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\my_projects\\gpt2_sentiment\\wandb\\run-20231025_213915-4rtt0y7n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maxfil333/huggingface/runs/4rtt0y7n' target=\"_blank\">prime-sound-20</a></strong> to <a href='https://wandb.ai/maxfil333/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maxfil333/huggingface' target=\"_blank\">https://wandb.ai/maxfil333/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maxfil333/huggingface/runs/4rtt0y7n' target=\"_blank\">https://wandb.ai/maxfil333/huggingface/runs/4rtt0y7n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3750' max='3750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3750/3750 22:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.848200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.775700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.752600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.695500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.746300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.720500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.704200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.733700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.712700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.696000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.713000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.688100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.641000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.619100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.669700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.639200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.646900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.675500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>3.618400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>3.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>3.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>3.614500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>3.599200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>3.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>3.628800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>3.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>3.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.651100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>3.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.577700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>3.591700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>3.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>3.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.572800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>3.586700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>3.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>3.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>3.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>3.605900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>3.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>3.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>3.577900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>3.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>3.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>3.602300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.591000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>3.581700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>3.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>3.611600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3750, training_loss=3.647882454427083, metrics={'train_runtime': 1373.2032, 'train_samples_per_second': 21.847, 'train_steps_per_second': 2.731, 'total_flos': 3913529988096000.0, 'train_loss': 3.647882454427083, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115fc818-0117-4501-a3e6-7201cacef112",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f243012e-03f9-4982-88ba-e122d8a7b1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie is really good. I think that if you watch it on DVD you will see a little bit more of it, especially the ending. The story is well written. The movie is also a good story, but it doesn't really do it justice in the end. The acting is good, and the movie gets more and more realistic. I hope that the people who watched the movie will find it a little bit better, since it is a very funny movie. The movie is good for the budget and its a good way to get your money's worth. The movie is very good for the fact that it is a movie, but I hope that it gets a little bit better in the future. I hope that the people watching the movie will find it a little bit better, since it is a funny movie, but I hope that the people watching the movie will find it a little bit better, because it is a funny movie, and because it is a very funny movie. And I hope\n"
     ]
    }
   ],
   "source": [
    "text = \"This movie is\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=2,\n",
    "                        temperature=1.5,\n",
    "                        top_p=0.9,\n",
    "                        max_length=200,\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "802a6609-0946-4fa2-a972-8117b6160b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is definetely sad when a movie is made without even a single mention of the actual story. The idea that it would have been better if all the plot elements were explained in a single, concise, and coherent piece of film is simply not true. I am not sure if this is a coincidence or a combination of the two, but it is disappointing that this movie was never given a chance to truly shine in any form.<br /><br />In the end, I can only comment that the movie was a disappointment and a shame. If you are a fan of the genre of video games and want to see a better version of this masterpiece, go for it. If you are a fan of any other genre, you will love this movie. It's an entertaining, enjoyable, and enjoyable experience. <br /><br />I will admit though that I have had a few criticisms about the movie, and I will try to respond to them in the next post. But, if\n"
     ]
    }
   ],
   "source": [
    "text = \"It is definetely\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, \n",
    "                        do_sample=True,\n",
    "                        num_beams=2,\n",
    "                        temperature=1.5,\n",
    "                        top_p=0.9,\n",
    "                        max_length=200,\n",
    "                        )\n",
    "\n",
    "generated_text = list(map(tokenizer.decode, out))[0]\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd2e930-6d9a-46fe-ac7a-7a62fb181d6a",
   "metadata": {},
   "source": [
    "### pushing to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a55e8fe0-fe10-4228-a2c7-0068777789a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0207a120b86444c4a16f0e68620597a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/maxfil333/gpt2_imdb_generator/commit/b0de0e31c514bd46533798c1aa334f3e907cf3d0', commit_message='Upload model', commit_description='', oid='b0de0e31c514bd46533798c1aa334f3e907cf3d0', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('gpt2_imdb_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "815aef87-f6a8-45ee-9746-f4ea4cf46154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/maxfil333/gpt2_imdb_generator/commit/7d604a42d1d84db56128697d4b056bc0ed0ebcb4', commit_message='Upload tokenizer', commit_description='', oid='7d604a42d1d84db56128697d4b056bc0ed0ebcb4', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('gpt2_imdb_generator')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
